{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SWAQ_QC_CODE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giuliaulpiani/SWAQ/blob/main/SWAQ_QC_CODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDiidUhphE19"
      },
      "source": [
        "This is the code to produce the two quality-controlled SWAQ datafiles (csv format) as in the TERN repository:\n",
        "\n",
        "https://portal.tern.org.au/schools-weather-air-sydney-nsw/22077\n",
        "\n",
        "Cite as:\n",
        "\n",
        "Hart, M. , Maharaj, A. , Di Virgilio, G. , Ulpiani, G. (2021): Schools Weather and Air Quality (SWAQ) – Quality Controlled Urban Dataset – Sydney (NSW). Version 1.0.0. Terrestrial Ecosystem Research Network (TERN). Dataset. https://doi.org/10.5281/zenodo.5016296\n",
        "\n",
        "It is written in very plain Python language for use also by beginners in Google Colab, a collaborative environment that runs in Google Drive.\n",
        "When using the code:\n",
        "- create a folder in Drive containing the code itself and the dummy_swaq_data.csv dataset. This dataset contains approximately 2-month data in the original format retrieved from the SWAQ Cloud, prior to time alignment; \n",
        "- make sure to update the project_folder in the \"Import packages\" section (first section) accordingly.\n",
        "\n",
        "In addition to “YYYY-MM-DD_Raw.csv” where flags supplement but do not alter the original data and “YYYY-MM-DD_Cleaned.csv” that contains a ready-to-use cleaned dataset, the code generates the \"swaq_data_aligned&resampled.xlsx\" Excel file with the original dataset aligned in time (no quality control).\n",
        "\n",
        "NB: to avoid any issues, please do not alter any names in the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UJF1kV4KlZb",
        "outputId": "1a4d9044-65b0-493f-cf06-fe04c0d0f0ef"
      },
      "source": [
        "# Import packages\n",
        "! pip install mpu\n",
        "! pip install XlsxWriter\n",
        "import os\n",
        "import pandas as pd \n",
        "import datetime\n",
        "from calendar import monthrange\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "import mpu\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "project_folder = \"/content/gdrive/My Drive/Colab Notebooks/SWAQ/PUBLIC CODE/\"\n",
        "os.chdir(project_folder)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mpu in /usr/local/lib/python3.7/dist-packages (0.23.1)\n",
            "Requirement already satisfied: XlsxWriter in /usr/local/lib/python3.7/dist-packages (3.0.1)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY9EGkIBKlZd"
      },
      "source": [
        "# ALIGN STATIONS IN TIME AND RESAMPLE TO AVOID GAPS IN TIME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZILP8fUiKlZe"
      },
      "source": [
        "# function to verify if there is any gap in the timeline. It returns \"check passed\" if no time misalignment is recorded\n",
        "# and \"WARNING: MISSING DATA\" if the timeline is broken\n",
        "def checkmissing(df):\n",
        "    df['Time'] = df.index\n",
        "    # calculate the time difference row by row and add it as a column\n",
        "    df['deltat'] = (df['Time']-df['Time'].shift()).fillna(pd.Timedelta('0 days'))\n",
        "    # express the difference in minutes and add it as a column\n",
        "    df['ans'] = df['deltat'].apply(lambda x: x  / np.timedelta64(1,'m')).astype('int64') % (24*60)\n",
        "    # print the count of unique values to see how many where different from 20 ± 1 minutes (sampling rate)\n",
        "    count=(df['ans'].value_counts())\n",
        "    print (count)\n",
        "    okvalues=[0,19,20,21]\n",
        "    total=0\n",
        "    for z in okvalues:\n",
        "        if z in count.keys():\n",
        "            total+=count[z]\n",
        "    totalok=total/len(df)*100\n",
        "    if totalok==100:\n",
        "        str='check passed'\n",
        "        test=0\n",
        "    else:\n",
        "        str='WARNING: MISSING DATA'\n",
        "        test=1\n",
        "    print(str)\n",
        "    df.drop(columns=['Time', 'deltat','ans'],inplace=True)\n",
        "    return test"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knh2hinKKlZe"
      },
      "source": [
        "# import SWAQ datafile \n",
        "fpath = project_folder+'dummy_swaq_data.csv'\n",
        "data = pd.read_csv(fpath,index_col=[0],parse_dates=True,dayfirst=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0PKXaAJKlZf"
      },
      "source": [
        "# Use conventional names for columns\n",
        "oldnames=[\"AirTemp\",\"AirHum\",\"AirPres\",\"WindDir\",\"WindSpeed\"]\n",
        "newnames=[\"T\",\"RH\",\"p\",\"wd\",\"ws\"]\n",
        "for scroll in range(len(oldnames)):\n",
        "    data.columns = data.columns.str.replace(oldnames[scroll], newnames[scroll])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ5cQqutKlZf"
      },
      "source": [
        "# Get stations names by looking for unique elements in column name\n",
        "stations = [col[:col.index(\"_\")] for col in data.columns]\n",
        "stations=np.asarray(stations)\n",
        "stations=np.unique(stations)\n",
        "# Split the SWAQ locations between those measuring meteo only (metonly) and those measuring air quality too (metoaq)\n",
        "metonly=['DULW','KELL','NARE','TARE','NEWT']\n",
        "# Get the ramaining locations by subtraction\n",
        "listations=stations.tolist()\n",
        "metoaq=list(set(listations)-set(metonly))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0KVICkOrxhH"
      },
      "source": [
        "# Select the station having the longest time recording and insert it at the beginning of the list of stations. The timestamp of this station will be used to time-align the others.\n",
        "first_st=data.iloc[-1,:].first_valid_index().split('_')[0]\n",
        "listations.remove(first_st)\n",
        "listations.insert(0, first_st)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48Ld6F4UKlZf"
      },
      "source": [
        "# Append all subdatasets related to each station into a list, set the Timestamp as datetime and then as index. \n",
        "# Use \"concat\" to merge all dataset along the horizontal axis (axis=1) to align them in time.\n",
        "mylist=[]\n",
        "for i in range(len(listations)):\n",
        "  subdata = data.filter(regex=listations[i])\n",
        "  timei=(listations[i]+\"_Timestamp\")\n",
        "  subdata[timei] = pd.to_datetime(subdata[timei],dayfirst=True)\n",
        "  subdata = subdata.set_index(subdata[timei])\n",
        "  subdata = subdata.loc[~subdata.index.duplicated(keep='first')]\n",
        "  mylist.append(subdata)           \n",
        "swdataor=pd.concat(mylist,axis=1)\n",
        "swdataor['Time'] = pd.to_datetime(swdataor.index)\n",
        "swdataor = swdataor.set_index(pd.DatetimeIndex(swdataor['Time']))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bekW8LhyKlZg",
        "outputId": "74fc5978-5281-4810-b786-2b7b6423c691"
      },
      "source": [
        "# Check time gaps\n",
        "test=checkmissing(swdataor)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20    8980\n",
            "5        5\n",
            "15       1\n",
            "0        1\n",
            "Name: ans, dtype: int64\n",
            "WARNING: MISSING DATA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pv6Tf1WKlZh",
        "outputId": "17562267-9de6-438f-c221-368576f79018"
      },
      "source": [
        "# If time gaps, apply resampling on Time column with frequency equal to measurement time step (20 mins). \n",
        "# If still gaps send a warning\n",
        "swdata=swdataor.copy() \n",
        "if test==1:\n",
        "    swdata['Time'] = swdata.index\n",
        "    swdata=swdata.resample('20Min', on='Time').first()\\\n",
        "           .drop('Time', 1)\n",
        "    test=checkmissing(swdata)\n",
        "    if test==1:\n",
        "        print('WARNING: THE DATA IS STILL SHOWING TIME GAPS. MANUAL CHECK NEEDED')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20    8982\n",
            "0        1\n",
            "Name: ans, dtype: int64\n",
            "check passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWtpgnlSKlZh"
      },
      "source": [
        "# write Excel File  \n",
        "swdata = swdata[swdata.columns.drop(list(swdata.filter(regex='Timestamp')))]\n",
        "filename=fpath.split('.')[0][-9:-1]+fpath.split('.')[0][-1]\n",
        "out_path = project_folder+filename+\"_aligned&resampled.xlsx\"\n",
        "writer = pd.ExcelWriter(out_path , engine='xlsxwriter')\n",
        "swdata.to_excel(writer, sheet_name='Sheet1')\n",
        "writer.save()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuBUow5HKlZh"
      },
      "source": [
        "# QUALITY CONTROL & FLAGGING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "ytQ_ILXwKlZh",
        "outputId": "8cfd1fc6-8ed0-4840-b0dd-2ad551d0d0b1"
      },
      "source": [
        "# Recall aligned and resampled dataset if necessary\n",
        "datapath = project_folder+filename+\"_aligned&resampled.xlsx\"\n",
        "swdataor = pd.read_excel(datapath)\n",
        "# Set timestamp as dataframe index\n",
        "swdataor = swdataor.set_index(['Time'])\n",
        "swdataor.head(3)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BROO_NO2</th>\n",
              "      <th>BROO_SO2</th>\n",
              "      <th>BROO_CO</th>\n",
              "      <th>BROO_O3</th>\n",
              "      <th>BROO_PM25</th>\n",
              "      <th>BROO_PM10</th>\n",
              "      <th>BROO_T</th>\n",
              "      <th>BROO_RH</th>\n",
              "      <th>BROO_p</th>\n",
              "      <th>BROO_Rain</th>\n",
              "      <th>BROO_wd</th>\n",
              "      <th>BROO_ws</th>\n",
              "      <th>DULW_T</th>\n",
              "      <th>DULW_RH</th>\n",
              "      <th>DULW_p</th>\n",
              "      <th>DULW_Rain</th>\n",
              "      <th>DULW_wd</th>\n",
              "      <th>DULW_ws</th>\n",
              "      <th>DULW_Solarrad</th>\n",
              "      <th>GLEN_NO2</th>\n",
              "      <th>GLEN_SO2</th>\n",
              "      <th>GLEN_CO</th>\n",
              "      <th>GLEN_O3</th>\n",
              "      <th>GLEN_PM25</th>\n",
              "      <th>GLEN_PM10</th>\n",
              "      <th>GLEN_T</th>\n",
              "      <th>GLEN_RH</th>\n",
              "      <th>GLEN_p</th>\n",
              "      <th>GLEN_Rain</th>\n",
              "      <th>GLEN_wd</th>\n",
              "      <th>GLEN_ws</th>\n",
              "      <th>KELL_T</th>\n",
              "      <th>KELL_RH</th>\n",
              "      <th>KELL_p</th>\n",
              "      <th>KELL_Rain</th>\n",
              "      <th>KELL_wd</th>\n",
              "      <th>KELL_ws</th>\n",
              "      <th>KURN_NO2</th>\n",
              "      <th>KURN_SO2</th>\n",
              "      <th>KURN_CO</th>\n",
              "      <th>...</th>\n",
              "      <th>NARE_p</th>\n",
              "      <th>NARE_Rain</th>\n",
              "      <th>NARE_wd</th>\n",
              "      <th>NARE_ws</th>\n",
              "      <th>NEWT_T</th>\n",
              "      <th>NEWT_RH</th>\n",
              "      <th>NEWT_p</th>\n",
              "      <th>NEWT_Rain</th>\n",
              "      <th>NEWT_wd</th>\n",
              "      <th>NEWT_ws</th>\n",
              "      <th>OEHS_NO2</th>\n",
              "      <th>OEHS_SO2</th>\n",
              "      <th>OEHS_CO</th>\n",
              "      <th>OEHS_O3</th>\n",
              "      <th>OEHS_PM25</th>\n",
              "      <th>OEHS_PM10</th>\n",
              "      <th>OEHS_T</th>\n",
              "      <th>OEHS_RH</th>\n",
              "      <th>OEHS_p</th>\n",
              "      <th>OEHS_Rain</th>\n",
              "      <th>OEHS_wd</th>\n",
              "      <th>OEHS_ws</th>\n",
              "      <th>TARE_T</th>\n",
              "      <th>TARE_RH</th>\n",
              "      <th>TARE_p</th>\n",
              "      <th>TARE_Rain</th>\n",
              "      <th>TARE_wd</th>\n",
              "      <th>TARE_ws</th>\n",
              "      <th>UNSW_NO2</th>\n",
              "      <th>UNSW_SO2</th>\n",
              "      <th>UNSW_CO</th>\n",
              "      <th>UNSW_O3</th>\n",
              "      <th>UNSW_PM25</th>\n",
              "      <th>UNSW_PM10</th>\n",
              "      <th>UNSW_T</th>\n",
              "      <th>UNSW_RH</th>\n",
              "      <th>UNSW_p</th>\n",
              "      <th>UNSW_Rain</th>\n",
              "      <th>UNSW_wd</th>\n",
              "      <th>UNSW_ws</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2019-09-01 00:00:00</th>\n",
              "      <td>0.023</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.063</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>2.5</td>\n",
              "      <td>4.4</td>\n",
              "      <td>18.6</td>\n",
              "      <td>58.5</td>\n",
              "      <td>1018.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>205.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.015</td>\n",
              "      <td>1.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>16.1</td>\n",
              "      <td>63.0</td>\n",
              "      <td>1001.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>338.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.092</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.046</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>3.5</td>\n",
              "      <td>7.6</td>\n",
              "      <td>17.0</td>\n",
              "      <td>56.4</td>\n",
              "      <td>1016.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>347.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.012</td>\n",
              "      <td>3.4</td>\n",
              "      <td>8.5</td>\n",
              "      <td>17.5</td>\n",
              "      <td>56.8</td>\n",
              "      <td>1015.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>262.0</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-09-01 00:20:00</th>\n",
              "      <td>0.023</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.029</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>2.9</td>\n",
              "      <td>4.5</td>\n",
              "      <td>18.6</td>\n",
              "      <td>60.4</td>\n",
              "      <td>1018.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>356.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.020</td>\n",
              "      <td>1.7</td>\n",
              "      <td>3.1</td>\n",
              "      <td>16.6</td>\n",
              "      <td>58.6</td>\n",
              "      <td>1001.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>335.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.087</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.262</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.002</td>\n",
              "      <td>3.6</td>\n",
              "      <td>9.3</td>\n",
              "      <td>18.0</td>\n",
              "      <td>53.7</td>\n",
              "      <td>1016.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.014</td>\n",
              "      <td>3.3</td>\n",
              "      <td>8.6</td>\n",
              "      <td>18.0</td>\n",
              "      <td>55.8</td>\n",
              "      <td>1014.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>270.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-09-01 00:40:00</th>\n",
              "      <td>0.018</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.2</td>\n",
              "      <td>4.5</td>\n",
              "      <td>19.5</td>\n",
              "      <td>55.2</td>\n",
              "      <td>1018.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>324.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.020</td>\n",
              "      <td>1.5</td>\n",
              "      <td>3.4</td>\n",
              "      <td>18.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>1000.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.078</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.357</td>\n",
              "      <td>0.043</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>3.9</td>\n",
              "      <td>9.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>50.3</td>\n",
              "      <td>1016.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.017</td>\n",
              "      <td>3.4</td>\n",
              "      <td>10.2</td>\n",
              "      <td>18.4</td>\n",
              "      <td>53.4</td>\n",
              "      <td>1014.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 115 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                     BROO_NO2  BROO_SO2  BROO_CO  ...  UNSW_Rain  UNSW_wd  UNSW_ws\n",
              "Time                                              ...                             \n",
              "2019-09-01 00:00:00     0.023     0.025    0.063  ...        0.0    262.0      0.6\n",
              "2019-09-01 00:20:00     0.023     0.021    0.029  ...        0.0    270.0      1.0\n",
              "2019-09-01 00:40:00     0.018     0.007    0.048  ...        0.0     44.0      0.6\n",
              "\n",
              "[3 rows x 115 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UUg1UGfKlZi"
      },
      "source": [
        "# Keep track of original dataset and initialize cleaned dataset\n",
        "swdata = swdataor.copy()\n",
        "sw_clean = swdataor.copy()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn7jlt8MKlZi"
      },
      "source": [
        "# Correct NEWT_wd by 180 degrees if before 26 Mar 2021, 11:30 am\n",
        "mask = swdata.index <= '2021-03-26 11:30:00'\n",
        "swdata.loc[mask,'NEWT_wd'] = np.where(swdata.loc[mask,'NEWT_wd']+180.0 > 360, swdata.loc[mask,'NEWT_wd']-180.0, swdata.loc[mask,'NEWT_wd']+180.0)\n",
        "sw_clean.loc[mask,'NEWT_wd'] = np.where(sw_clean.loc[mask,'NEWT_wd']+180.0 > 360, sw_clean.loc[mask,'NEWT_wd']-180.0, sw_clean.loc[mask,'NEWT_wd']+180.0)\n",
        "# Correct all RH measurements by applying a positive offset of (100-94.7), \n",
        "# where 94.7 is the absolute maximum measured on the 2019-31Jan2021 dataset\n",
        "swdata.loc[:,swdata.filter(regex='_RH').columns]=swdata.loc[:,swdata.filter(regex='_RH').columns]+100-94.7\n",
        "sw_clean.loc[:,sw_clean.filter(regex='_RH').columns]=sw_clean.loc[:,sw_clean.filter(regex='_RH').columns]+100-94.7\n",
        "# Remove any column containing \"Solarrad\" (unknown measurement in Dulwich Hill)\n",
        "swdata=swdata[swdata.columns.drop(list(swdata.filter(regex='Solarrad')))]\n",
        "sw_clean=sw_clean[sw_clean.columns.drop(list(sw_clean.filter(regex='Solarrad')))]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVAhPlrRKlZi"
      },
      "source": [
        "# Create a flag column for each measured parameter\n",
        "columns=swdata.columns\n",
        "for col in columns:\n",
        "    label=col+'_Flags'\n",
        "    swdata[label]=['' for i in range(swdata.shape[0])]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sFbtouAKlZj"
      },
      "source": [
        "##### CONTINUITY TEST: Flag missing rows\n",
        "##### Set flag columns to \"STF4.1\" if data is missing\n",
        "for col in columns:\n",
        "  varname=col\n",
        "  flagname=col+'_Flags'\n",
        "  for ind in swdata.index:\n",
        "      if pd.isnull(swdata.loc[ind,varname]):\n",
        "          swdata.loc[ind,flagname] = 'STF4.1;'"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JTVckfOKlZj"
      },
      "source": [
        "##### FIXED RANGE TESTS: Remove non-physical values\n",
        "meteovar=['T','RH','p','Rain','ws','wd'];oaqvar=['NO2','SO2','CO','O3','PM25','PM10']\n",
        "met_lowerbound=[-52.0,0.0,500.0,0.0,0.0,0];oaq_lowerbound=[0.000,0.000,0.0000,0.000,0.000,0.000]\n",
        "met_upperbound=[60.0,100,1100.0,200,60.0,360];oaq_upperbound=[2000/1000,2000/1000,10000/1000,2000/1000,2000,3276] # 5000 replaced by 3276 after perusing dataset\n",
        "for st in stations: \n",
        "    for var in range(len(meteovar)): \n",
        "        varname=st+'_'+meteovar[var]\n",
        "        flagname=st+'_'+meteovar[var]+'_Flags'\n",
        "        indices_lo=list(np.where(swdata[varname] < met_lowerbound[var])[0])\n",
        "        indices_up=list(np.where(swdata[varname] > met_upperbound[var])[0])\n",
        "        indices=np.concatenate((indices_lo, indices_up)).astype(int)\n",
        "        swdata.iloc[indices,swdata.columns.get_loc(flagname)] = swdata.iloc[indices,swdata.columns.get_loc(flagname)]+'STF2.1;'\n",
        "        if meteovar[var] in ['RH','Rain','ws','wd']:\n",
        "            sw_clean.iloc[indices_lo,sw_clean.columns.get_loc(varname)] = met_lowerbound[var]\n",
        "        if meteovar[var] in ['RH','wd']:\n",
        "            sw_clean.iloc[indices_up,sw_clean.columns.get_loc(varname)] = met_upperbound[var]\n",
        "        del indices_up; del indices_lo; del indices\n",
        "for st in metoaq:        \n",
        "    for var in range(len(oaqvar)):\n",
        "        varname=st+'_'+oaqvar[var]\n",
        "        flagname=st+'_'+oaqvar[var]+'_Flags'\n",
        "        indices_lo=list(np.where(swdata[varname] < oaq_lowerbound[var])[0])\n",
        "        indices_up=list(np.where(swdata[varname] > oaq_upperbound[var])[0])\n",
        "        indices=np.concatenate((indices_lo, indices_up)).astype(int)\n",
        "        swdata.iloc[indices,swdata.columns.get_loc(flagname)] = swdata.iloc[indices,swdata.columns.get_loc(flagname)]+'STF2.1;'\n",
        "        sw_clean.iloc[indices_lo,sw_clean.columns.get_loc(varname)] = oaq_lowerbound[var]\n",
        "        sw_clean.iloc[indices_up,sw_clean.columns.get_loc(varname)] = np.nan"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiq0gxZwKlZk"
      },
      "source": [
        "##### INTERNAL CONSISTENCY TESTS: Remove inconsistent values as for inter-parameter associations\n",
        "for st in stations: \n",
        "    varname1=st+'_ws';varname2=st+'_wd'\n",
        "    flagname1=st+'_ws_Flags';flagname2=st+'_wd_Flags'\n",
        "    indices=list(np.where((swdata[varname1] == 0) & (swdata[varname2] != 0))[0])\n",
        "    sw_clean.iloc[indices,sw_clean.columns.get_loc(varname1)] = np.nan\n",
        "    sw_clean.iloc[indices,sw_clean.columns.get_loc(varname2)] = np.nan\n",
        "    swdata.iloc[indices,swdata.columns.get_loc(flagname)] = swdata.iloc[indices,swdata.columns.get_loc(flagname)]+'STF2.2;'"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcoG5HLxKlZk"
      },
      "source": [
        "##### PERSISTENCE TESTS according to Meek and Hatfield (1994): flag consecutive identical readings over 3 hours\n",
        "# check if preceding value is the same using .shift() function\n",
        "for col in columns:\n",
        "    varname=col[0:13]\n",
        "    flagname=col[0:13]+'_Flags'\n",
        "    swdata['same_as_shift'] = swdata[varname].shift() != swdata[varname] \n",
        "    for name, group in swdata.groupby(swdata.same_as_shift.cumsum()):\n",
        "        if (len(group) > 9) & (not (\"Rain\") in varname):\n",
        "            swdata.loc[group.index,flagname] = swdata.loc[group.index,flagname] + 'STF3;'\n",
        "            sw_clean.loc[group.index,varname] = np.nan\n",
        "    swdata = swdata.drop(columns=['same_as_shift'])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH3xlCLSKlZk"
      },
      "source": [
        "##### DYNAMIC RANGE TESTS (site-specific extremes): for each station and parameter, verify if \n",
        "##### the measured value is an outlier with respect to the monthly dataset of all stations \n",
        "#####(Outlier definition: <p25-1.5IQR or >p75+1.5IQR)\n",
        "# NB: d is a dictionary of outliers. The outmost key is a progressive index for each monthly period:\n",
        "# By printing d[1] one can access all outliers pertaining to the first month of data, broken down by parameter\n",
        "allvar = meteovar + oaqvar\n",
        "allvar=[x for x in allvar if x not in ['Rain','RH','wd']]\n",
        "allvar = [\"_\" + par for par in allvar]\n",
        "idx=1\n",
        "d = {}\n",
        "for name, group in swdata.groupby(pd.Grouper(freq=\"M\")):\n",
        "    d[idx] = {}\n",
        "    percent=len(group)/(monthrange(name.year, name.month)[1]*24*3)*100\n",
        "    if percent < 90:\n",
        "        swdata.loc[group.index,swdata.filter(regex='Flags').columns]=swdata.loc[group.index,swdata.filter(regex='Flags').columns]+\"STF4.2;\"\n",
        "        sw_clean.loc[group.index,:]=np.nan\n",
        "    else:\n",
        "        for var in allvar:\n",
        "            d[idx]['Period'] = str(name.month)+'/'+str(name.year)\n",
        "            d[idx][var[1:]]= {}\n",
        "            # Find the 75th and 25th percentile of the parameter in object (var) by looking at all stations\n",
        "            var_cols = [col for col in swdata.columns if (var in col) & (not (\"Flags\") in col)]\n",
        "            # Calculate outliers in cleaned dataset (OR remove previous flags???)\n",
        "            q75,q25 = np.nanpercentile(sw_clean.loc[group.index,var_cols],[75,25])\n",
        "            # Calculate the interquartile range\n",
        "            intr_qr = q75-q25\n",
        "            # Apply the definition of outlier to define the thresholds (max and min)\n",
        "            max = q75+(1.5*intr_qr)\n",
        "            min = q25-(1.5*intr_qr)   \n",
        "            d[idx][var[1:]]['Upper'] = round(max,4)\n",
        "            d[idx][var[1:]]['Lower'] = round(min,4)\n",
        "            for i in var_cols:\n",
        "                flagname=i+'_Flags'\n",
        "                if idx==1:\n",
        "                    indices_up=list(np.where(swdata.loc[group.index,i] > max)[0])\n",
        "                    indices_lo=list(np.where(swdata.loc[group.index,i] < min)[0])\n",
        "                else:\n",
        "                    indices_up=list(np.where(swdata.loc[group.index,i] > max)[0])+np.argwhere(swdata.index < group.index[0]).flatten()[-1]+1\n",
        "                    indices_lo=list(np.where(swdata.loc[group.index,i] < min)[0])+np.argwhere(swdata.index < group.index[0]).flatten()[-1]+1\n",
        "                indices=np.concatenate((indices_up, indices_lo)).astype(int)\n",
        "                swdata.iloc[indices,swdata.columns.get_loc(flagname)] = swdata.iloc[indices,swdata.columns.get_loc(flagname)]+'STF1.1;'\n",
        "                del indices_up; del indices_lo; del indices\n",
        "    idx+=1"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mzQUnKHKlZl"
      },
      "source": [
        "##### STEP TESTS (site-specific extremes): for each station and parameter, verify if \n",
        "##### the step from previous measurement is an outlier with respect to the steps of all stations across the same month\n",
        "#####(Wider outlier definition: <p25-3IQR or >p75+3IQR)\n",
        "# NB: ds is a dictionary of outliers. The outmost key is a progressive index for each monthly period:\n",
        "# By printing d[1] one can access all outliers pertaining to the first month of data, broken down by parameter\n",
        "\n",
        "for col in columns:\n",
        "    label=col+'_Delta'\n",
        "    swdata[label]=abs(swdata[col] - swdata[col].shift(1))\n",
        "    sw_clean[label]=abs(sw_clean[col] - sw_clean[col].shift(1))    \n",
        "    \n",
        "allvar = meteovar + oaqvar\n",
        "allvar=[x for x in allvar if x not in ['Rain','wd']]\n",
        "allvar = [\"_\" + par for par in allvar]\n",
        "idx=1\n",
        "ds = {}\n",
        "for name, group in swdata.groupby(pd.Grouper(freq=\"M\")):\n",
        "    ds[idx] = {}\n",
        "    percent=len(group)/(monthrange(name.year, name.month)[1]*24*3)*100\n",
        "    if percent >= 90:\n",
        "        for var in allvar:\n",
        "            ds[idx]['Period'] = str(name.month)+'/'+str(name.year)\n",
        "            ds[idx][var[1:]]= {}\n",
        "            # Find the 75th and 25th percentile of the parameter in object (var) by looking at all stations\n",
        "            var_cols = [col for col in swdata.columns if (var in col) & ((\"Delta\") in col) & (not (\"Flags\") in col)]\n",
        "            # Calculate outliers in cleaned dataset (OR remove previous flags???)\n",
        "            q75,q25 = np.nanpercentile(sw_clean.loc[group.index,var_cols],[75,25])\n",
        "            # Calculate the interquartile range\n",
        "            intr_qr = q75-q25\n",
        "            # Apply the definition of outlier to define the thresholds (max)\n",
        "            max = q75+(3*intr_qr)  \n",
        "            ds[idx][var[1:]] = round(max,4)\n",
        "            for i in var_cols:\n",
        "                flagname=i.replace('_Delta','')+'_Flags'\n",
        "                if idx==1:\n",
        "                    indices=list(np.where(swdata.loc[group.index,i] > max)[0])\n",
        "                else:\n",
        "                    indices=list(np.where(swdata.loc[group.index,i] > max)[0])+np.argwhere(swdata.index < group.index[0]).flatten()[-1]+1\n",
        "                # Shall include line to check if indices in not null?\n",
        "                if len(indices)==0:\n",
        "                  continue\n",
        "                else:\n",
        "                  swdata.iloc[indices,swdata.columns.get_loc(flagname)] = swdata.iloc[indices,swdata.columns.get_loc(flagname)]+'STF1.2;'\n",
        "                del indices\n",
        "    idx+=1\n",
        "swdata = swdata[swdata.columns.drop(list(swdata.filter(regex='Delta')))]\n",
        "sw_clean = sw_clean[sw_clean.columns.drop(list(sw_clean.filter(regex='Delta')))]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMHTG8GWKlZm"
      },
      "source": [
        "##### APPLY COMBINATORIAL FLAGS and set data to NaN in cleaned dataset only if both dynamic range test and step test are simultaneously failed\n",
        "##### NB: allow even more than 1 hour to complete, depending on the size of the dataset. For the dummy_swaq_data.csv file this section would take approx. 15 minutes.\n",
        "for col in columns:\n",
        "    varname=col[0:13]\n",
        "    flagname=col[0:13]+'_Flags'\n",
        "    for ind in swdata.index:\n",
        "        if swdata.loc[ind,flagname]=='':\n",
        "            swdata.loc[ind,flagname]=swdata.loc[ind,flagname]+'STF0;CF0'\n",
        "        elif 'STF1.1;STF1.2;' in swdata.loc[ind,flagname]:# the \"contains\" function is used otherwise cells that have failed also other tests would not be removed\n",
        "            swdata.loc[ind,flagname]=swdata.loc[ind,flagname]+'CF1;'\n",
        "            sw_clean.loc[ind,varname] = np.nan\n",
        "        elif ('STF2.1;' in swdata.loc[ind,flagname]) | ('STF2.2;' in swdata.loc[ind,flagname]):\n",
        "            swdata.loc[ind,flagname]=swdata.loc[ind,flagname]+'CF2'\n",
        "        elif swdata.loc[ind,flagname]=='STF3':\n",
        "            swdata.loc[ind,flagname]=swdata.loc[ind,flagname]+'CF3'\n",
        "        elif ('STF4.1;' in swdata.loc[ind,flagname]) | ('STF4.2;' in swdata.loc[ind,flagname]):\n",
        "            swdata.loc[ind,flagname]=swdata.loc[ind,flagname]+'CF4'"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nJLtdHIKlZm"
      },
      "source": [
        "# write csv files with datetime format to ISO 8601\n",
        "swd=swdata; sw_c=sw_clean\n",
        "swd.index = swd.index.to_series().apply(datetime.datetime.isoformat)\n",
        "sw_c.index = sw_c.index.to_series().apply(datetime.datetime.isoformat)\n",
        "# Get last readings' timestamp and convert it into isoformat then build up filename\n",
        "filename1=swdata.index[-1].split('T')[0]+\"_Raw.csv\"\n",
        "filename2=swdata.index[-1].split('T')[0]+\"_Cleaned.csv\"\n",
        "swd.to_csv(filename1)\n",
        "sw_c.to_csv(filename2)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICCPjQIyJogx"
      },
      "source": [
        "END OF CODE"
      ]
    }
  ]
}